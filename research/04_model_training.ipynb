{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baf491a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6022be68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/lakshitjain/Developer/ML Projects/Brain-Tumor-MRI-Classification'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "21b406d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    root_dir : Path\n",
    "    trained_model_path:Path\n",
    "    base_model_path:Path\n",
    "    training_data:Path\n",
    "    param_epochs : int\n",
    "    param_batch_size : int\n",
    "    param_is_augmentation : bool\n",
    "    param_image_size : list\n",
    "    param_learning_rate : float\n",
    "    param_seed : int\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PrepareCallbacksConfig:\n",
    "    root_dir: Path\n",
    "    checkpoint_model_filepath: Path\n",
    "    tensorboard_root_log_dir: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "27be82eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brainTumorMRIClassification.constants import *\n",
    "from brainTumorMRIClassification.utils.common import read_yaml, create_directories\n",
    "import tensorflow as tf\n",
    "from brainTumorMRIClassification import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa906ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, config_filepath = CONFIG_FILE_PATH,params_filepath = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_prepare_callbacks_config(self) -> PrepareCallbacksConfig:\n",
    "        config=self.config.prepare_callbacks\n",
    "        model_ckpt_dir = os.path.dirname(config.checkpoint_model_filepath)\n",
    "        create_directories([Path(model_ckpt_dir), Path(config.tensorboard_root_log_dir)])\n",
    "        \n",
    "        return PrepareCallbacksConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            checkpoint_model_filepath=Path(config.checkpoint_model_filepath),\n",
    "            tensorboard_root_log_dir=Path(config.tensorboard_root_log_dir)\n",
    "        )\n",
    "    \n",
    "    def get_training_config(self)->TrainingConfig:\n",
    "        training=self.config.training\n",
    "        prepare_base_model=self.config.prepare_base_model\n",
    "        params=self.params\n",
    "        training_data = os.path.join(self.config.data_ingestion.unzip_dir)\n",
    "        create_directories([Path(training.root_dir)])\n",
    "\n",
    "        return TrainingConfig(\n",
    "            root_dir=Path(training.root_dir),\n",
    "            trained_model_path=Path(training.trained_model_path),\n",
    "            base_model_path=Path(prepare_base_model.base_model_path),\n",
    "            training_data=Path(training_data),\n",
    "            param_epochs=params.EPOCHS,\n",
    "            param_batch_size=params.BATCH_SIZE,\n",
    "            param_is_augmentation=params.AUGMENTATION,\n",
    "            param_image_size=params.IMAGE_SIZE,\n",
    "            param_learning_rate=params.LEARNING_RATE,\n",
    "            param_seed=params.SEED\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "55b8a0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PrepareCallback:\n",
    "    def __init__(self, config: PrepareCallbacksConfig):\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "    @property\n",
    "    def _create_early_stopping(self):\n",
    "        return tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='loss',\n",
    "            min_delta=1e-9,\n",
    "            patience=8,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def _create_reduce_lr(self):\n",
    "        return tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.3,\n",
    "            patience=5,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "    def get_all_callbacks(self):\n",
    "        return [\n",
    "            self._create_early_stopping,\n",
    "            self._create_reduce_lr\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc7e787",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training:\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def get_base_model(self):\n",
    "        self.model = tf.keras.models.load_model(\n",
    "            self.config.base_model_path\n",
    "        )\n",
    "        self.model.compile(\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=self.config.param_learning_rate, beta_1=0.869, beta_2=0.995),\n",
    "            loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "    def train_valid_generator(self):\n",
    "        train_dir = os.path.join(self.config.training_data, \"Training\")\n",
    "        test_dir = os.path.join(self.config.training_data, \"Testing\")\n",
    "        image_size = self.config.param_image_size\n",
    "        target_size = tuple(image_size[:2])  # Only height and width for flow_from_directory\n",
    "        batch_size = self.config.param_batch_size\n",
    "        SEED = self.config.param_seed\n",
    "        from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "        # Data augmentation and preprocessing for training\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            rotation_range=10,\n",
    "            brightness_range=(0.85, 1.15),\n",
    "            width_shift_range=0.002,\n",
    "            height_shift_range=0.002,\n",
    "            shear_range=12.5,\n",
    "            zoom_range=0,\n",
    "            horizontal_flip=True,\n",
    "            vertical_flip=False,\n",
    "            fill_mode=\"nearest\"\n",
    "        )\n",
    "        self.train_generator = train_datagen.flow_from_directory(\n",
    "            train_dir,\n",
    "            target_size=target_size,\n",
    "            batch_size=batch_size,\n",
    "            class_mode=\"categorical\",\n",
    "            seed=SEED\n",
    "        )\n",
    "\n",
    "        # No augmentation for test data, just rescaling\n",
    "        test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "        self.valid_generator = test_datagen.flow_from_directory(\n",
    "            test_dir,\n",
    "            target_size=target_size,\n",
    "            batch_size=batch_size,\n",
    "            class_mode=\"categorical\",\n",
    "            shuffle=False,\n",
    "            seed=SEED\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def save_model(path: Path, model: tf.keras.Model):\n",
    "        model.save(path)\n",
    "        logger.info(f\"Model saved at {path}\")\n",
    "\n",
    "    def train(self, callback_list: list):\n",
    "        self.steps_per_epoch = self.train_generator.samples // self.train_generator.batch_size\n",
    "        self.validation_steps = self.valid_generator.samples // self.valid_generator.batch_size\n",
    "\n",
    "        logger.info(\"Training Started\")\n",
    "        self.model.fit(\n",
    "            self.train_generator,\n",
    "            epochs=self.config.param_epochs,\n",
    "            steps_per_epoch=self.steps_per_epoch,\n",
    "            validation_data=self.valid_generator,\n",
    "            validation_steps=self.validation_steps,\n",
    "            callbacks=callback_list\n",
    "        )\n",
    "\n",
    "        self.save_model(\n",
    "            path=self.config.trained_model_path,\n",
    "            model=self.model\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ac0c6d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-24 19:08:21,066] : INFO : common : YAML file config/config.yaml loaded successfully.\n",
      "[2025-08-24 19:08:21,069] : INFO : common : YAML file params.yaml loaded successfully.\n",
      "[2025-08-24 19:08:21,069] : INFO : common : Created directory: artifacts\n",
      "[2025-08-24 19:08:21,070] : INFO : common : Created directory: artifacts/prepare_callbacks/checkpoint_dir\n",
      "[2025-08-24 19:08:21,070] : INFO : common : Created directory: artifacts/prepare_callbacks/tensorboard_log_dir\n",
      "[2025-08-24 19:08:21,071] : INFO : common : Created directory: artifacts/training\n",
      "[2025-08-24 19:08:21,069] : INFO : common : YAML file params.yaml loaded successfully.\n",
      "[2025-08-24 19:08:21,069] : INFO : common : Created directory: artifacts\n",
      "[2025-08-24 19:08:21,070] : INFO : common : Created directory: artifacts/prepare_callbacks/checkpoint_dir\n",
      "[2025-08-24 19:08:21,070] : INFO : common : Created directory: artifacts/prepare_callbacks/tensorboard_log_dir\n",
      "[2025-08-24 19:08:21,071] : INFO : common : Created directory: artifacts/training\n",
      "[2025-08-24 19:08:21,133] : WARNING : saving_utils : Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "[2025-08-24 19:08:21,133] : WARNING : saving_utils : Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Found 5712 images belonging to 4 classes.\n",
      "Found 5712 images belonging to 4 classes.\n",
      "Found 1311 images belonging to 4 classes.\n",
      "[2025-08-24 19:08:21,302] : INFO : 1380972102 : Training Started\n",
      "Found 1311 images belonging to 4 classes.\n",
      "[2025-08-24 19:08:21,302] : INFO : 1380972102 : Training Started\n",
      "Epoch 1/40\n",
      "Epoch 1/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 135ms/step - accuracy: 0.6206 - loss: 0.8332 - val_accuracy: 0.7516 - val_loss: 0.6815 - learning_rate: 0.0010\n",
      "Epoch 2/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 135ms/step - accuracy: 0.6206 - loss: 0.8332 - val_accuracy: 0.7516 - val_loss: 0.6815 - learning_rate: 0.0010\n",
      "Epoch 2/40\n",
      "\u001b[1m  1/178\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 119ms/step - accuracy: 0.8125 - loss: 0.2888"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lakshitjain/Developer/ML Projects/Brain-Tumor-MRI-Classification/venv/lib/python3.10/site-packages/keras/src/trainers/epoch_iterator.py:116: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8125 - loss: 0.2888 - val_accuracy: 0.7414 - val_loss: 0.7053 - learning_rate: 0.0010\n",
      "Epoch 3/40\n",
      "Epoch 3/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 141ms/step - accuracy: 0.7952 - loss: 0.5052 - val_accuracy: 0.7898 - val_loss: 0.5689 - learning_rate: 0.0010\n",
      "Epoch 4/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 141ms/step - accuracy: 0.7952 - loss: 0.5052 - val_accuracy: 0.7898 - val_loss: 0.5689 - learning_rate: 0.0010\n",
      "Epoch 4/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9062 - loss: 0.4078 - val_accuracy: 0.7859 - val_loss: 0.5835 - learning_rate: 0.0010\n",
      "Epoch 5/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9062 - loss: 0.4078 - val_accuracy: 0.7859 - val_loss: 0.5835 - learning_rate: 0.0010\n",
      "Epoch 5/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 155ms/step - accuracy: 0.8440 - loss: 0.3884 - val_accuracy: 0.7961 - val_loss: 0.5455 - learning_rate: 0.0010\n",
      "Epoch 6/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 155ms/step - accuracy: 0.8440 - loss: 0.3884 - val_accuracy: 0.7961 - val_loss: 0.5455 - learning_rate: 0.0010\n",
      "Epoch 6/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7188 - loss: 0.6324 - val_accuracy: 0.8281 - val_loss: 0.4463 - learning_rate: 0.0010\n",
      "Epoch 7/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7188 - loss: 0.6324 - val_accuracy: 0.8281 - val_loss: 0.4463 - learning_rate: 0.0010\n",
      "Epoch 7/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 161ms/step - accuracy: 0.8794 - loss: 0.3107 - val_accuracy: 0.8789 - val_loss: 0.3280 - learning_rate: 0.0010\n",
      "Epoch 8/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 161ms/step - accuracy: 0.8794 - loss: 0.3107 - val_accuracy: 0.8789 - val_loss: 0.3280 - learning_rate: 0.0010\n",
      "Epoch 8/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.8750 - loss: 0.3622 - val_accuracy: 0.8328 - val_loss: 0.4517 - learning_rate: 0.0010\n",
      "Epoch 9/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.8750 - loss: 0.3622 - val_accuracy: 0.8328 - val_loss: 0.4517 - learning_rate: 0.0010\n",
      "Epoch 9/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 182ms/step - accuracy: 0.9113 - loss: 0.2331 - val_accuracy: 0.8836 - val_loss: 0.3243 - learning_rate: 0.0010\n",
      "Epoch 10/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 182ms/step - accuracy: 0.9113 - loss: 0.2331 - val_accuracy: 0.8836 - val_loss: 0.3243 - learning_rate: 0.0010\n",
      "Epoch 10/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8750 - loss: 0.2104 - val_accuracy: 0.8680 - val_loss: 0.3468 - learning_rate: 0.0010\n",
      "Epoch 11/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8750 - loss: 0.2104 - val_accuracy: 0.8680 - val_loss: 0.3468 - learning_rate: 0.0010\n",
      "Epoch 11/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 192ms/step - accuracy: 0.9241 - loss: 0.2075 - val_accuracy: 0.8758 - val_loss: 0.3674 - learning_rate: 0.0010\n",
      "Epoch 12/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 192ms/step - accuracy: 0.9241 - loss: 0.2075 - val_accuracy: 0.8758 - val_loss: 0.3674 - learning_rate: 0.0010\n",
      "Epoch 12/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9062 - loss: 0.2225 - val_accuracy: 0.8391 - val_loss: 0.4712 - learning_rate: 0.0010\n",
      "Epoch 13/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9062 - loss: 0.2225 - val_accuracy: 0.8391 - val_loss: 0.4712 - learning_rate: 0.0010\n",
      "Epoch 13/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 217ms/step - accuracy: 0.9424 - loss: 0.1633 - val_accuracy: 0.8984 - val_loss: 0.3124 - learning_rate: 0.0010\n",
      "Epoch 14/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 217ms/step - accuracy: 0.9424 - loss: 0.1633 - val_accuracy: 0.8984 - val_loss: 0.3124 - learning_rate: 0.0010\n",
      "Epoch 14/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.9688 - loss: 0.0925 - val_accuracy: 0.8906 - val_loss: 0.3206 - learning_rate: 0.0010\n",
      "Epoch 15/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.9688 - loss: 0.0925 - val_accuracy: 0.8906 - val_loss: 0.3206 - learning_rate: 0.0010\n",
      "Epoch 15/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 226ms/step - accuracy: 0.9468 - loss: 0.1461 - val_accuracy: 0.9102 - val_loss: 0.2671 - learning_rate: 0.0010\n",
      "Epoch 16/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 226ms/step - accuracy: 0.9468 - loss: 0.1461 - val_accuracy: 0.9102 - val_loss: 0.2671 - learning_rate: 0.0010\n",
      "Epoch 16/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0611 - val_accuracy: 0.8984 - val_loss: 0.3106 - learning_rate: 0.0010\n",
      "Epoch 17/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0611 - val_accuracy: 0.8984 - val_loss: 0.3106 - learning_rate: 0.0010\n",
      "Epoch 17/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 220ms/step - accuracy: 0.9511 - loss: 0.1332 - val_accuracy: 0.9586 - val_loss: 0.1217 - learning_rate: 0.0010\n",
      "Epoch 18/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 220ms/step - accuracy: 0.9511 - loss: 0.1332 - val_accuracy: 0.9586 - val_loss: 0.1217 - learning_rate: 0.0010\n",
      "Epoch 18/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9375 - loss: 0.0995 - val_accuracy: 0.9594 - val_loss: 0.1145 - learning_rate: 0.0010\n",
      "Epoch 19/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9375 - loss: 0.0995 - val_accuracy: 0.9594 - val_loss: 0.1145 - learning_rate: 0.0010\n",
      "Epoch 19/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 207ms/step - accuracy: 0.9574 - loss: 0.1188 - val_accuracy: 0.9602 - val_loss: 0.1470 - learning_rate: 0.0010\n",
      "Epoch 20/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 207ms/step - accuracy: 0.9574 - loss: 0.1188 - val_accuracy: 0.9602 - val_loss: 0.1470 - learning_rate: 0.0010\n",
      "Epoch 20/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9375 - loss: 0.1372 - val_accuracy: 0.9563 - val_loss: 0.1487 - learning_rate: 0.0010\n",
      "Epoch 21/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9375 - loss: 0.1372 - val_accuracy: 0.9563 - val_loss: 0.1487 - learning_rate: 0.0010\n",
      "Epoch 21/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 197ms/step - accuracy: 0.9579 - loss: 0.1232 - val_accuracy: 0.9680 - val_loss: 0.1079 - learning_rate: 0.0010\n",
      "Epoch 22/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 197ms/step - accuracy: 0.9579 - loss: 0.1232 - val_accuracy: 0.9680 - val_loss: 0.1079 - learning_rate: 0.0010\n",
      "Epoch 22/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9688 - loss: 0.0522 - val_accuracy: 0.9664 - val_loss: 0.1050 - learning_rate: 0.0010\n",
      "Epoch 23/40\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9688 - loss: 0.0522 - val_accuracy: 0.9664 - val_loss: 0.1050 - learning_rate: 0.0010\n",
      "Epoch 23/40\n",
      "\u001b[1m 33/178\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 189ms/step - accuracy: 0.9773 - loss: 0.0697"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 19:14:56.291838: W tensorflow/core/framework/op_kernel.cc:1842] UNKNOWN: FileNotFoundError: [Errno 2] No such file or directory: 'artifacts/data_ingestion/brain-mri/Training/pituitary/Tr-pi_1397.jpg'\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/lakshitjain/Developer/ML Projects/Brain-Tumor-MRI-Classification/venv/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/Users/lakshitjain/Developer/ML Projects/Brain-Tumor-MRI-Classification/venv/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/Users/lakshitjain/Developer/ML Projects/Brain-Tumor-MRI-Classification/venv/lib/python3.10/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n",
      "    values = next(generator_state.get_iterator(iterator_id))\n",
      "\n",
      "  File \"/Users/lakshitjain/Developer/ML Projects/Brain-Tumor-MRI-Classification/venv/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py\", line 264, in _finite_generator\n",
      "    yield self._standardize_batch(self.py_dataset[i])\n",
      "\n",
      "  File \"/Users/lakshitjain/Developer/ML Projects/Brain-Tumor-MRI-Classification/venv/lib/python3.10/site-packages/keras/src/legacy/preprocessing/image.py\", line 68, in __getitem__\n",
      "    return self._get_batches_of_transformed_samples(index_array)\n",
      "\n",
      "  File \"/Users/lakshitjain/Developer/ML Projects/Brain-Tumor-MRI-Classification/venv/lib/python3.10/site-packages/keras/src/legacy/preprocessing/image.py\", line 313, in _get_batches_of_transformed_samples\n",
      "    img = image_utils.load_img(\n",
      "\n",
      "  File \"/Users/lakshitjain/Developer/ML Projects/Brain-Tumor-MRI-Classification/venv/lib/python3.10/site-packages/keras/src/utils/image_utils.py\", line 235, in load_img\n",
      "    with open(path, \"rb\") as f:\n",
      "\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'artifacts/data_ingestion/brain-mri/Training/pituitary/Tr-pi_1397.jpg'\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 34/178\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 197ms/step - accuracy: 0.9772 - loss: 0.0699"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 19:14:56.883519: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: UNKNOWN: FileNotFoundError: [Errno 2] No such file or directory: 'artifacts/data_ingestion/brain-mri/Training/pituitary/Tr-pi_1397.jpg'\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/lakshitjain/Developer/ML Projects/Brain-Tumor-MRI-Classification/venv/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/Users/lakshitjain/Developer/ML Projects/Brain-Tumor-MRI-Classification/venv/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/Users/lakshitjain/Developer/ML Projects/Brain-Tumor-MRI-Classification/venv/lib/python3.10/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n",
      "    values = next(generator_state.get_iterator(iterator_id))\n",
      "\n",
      "  File \"/Users/lakshitjain/Developer/ML Projects/Brain-Tumor-MRI-Classification/venv/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py\", line 264, in _finite_generator\n",
      "    yield self._standardize_batch(self.py_dataset[i])\n",
      "\n",
      "  File \"/Users/lakshitjain/Developer/ML Projects/Brain-Tumor-MRI-Classification/venv/lib/python3.10/site-packages/keras/src/legacy/preprocessing/image.py\", line 68, in __getitem__\n",
      "    return self._get_batches_of_transformed_samples(index_array)\n",
      "\n",
      "  File \"/Users/lakshitjain/Developer/ML Projects/Brain-Tumor-MRI-Classification/venv/lib/python3.10/site-packages/keras/src/legacy/preprocessing/image.py\", line 313, in _get_batches_of_transformed_samples\n",
      "    img = image_utils.load_img(\n",
      "\n",
      "  File \"/Users/lakshitjain/Developer/ML Projects/Brain-Tumor-MRI-Classification/venv/lib/python3.10/site-packages/keras/src/utils/image_utils.py\", line 235, in load_img\n",
      "    with open(path, \"rb\") as f:\n",
      "\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'artifacts/data_ingestion/brain-mri/Training/pituitary/Tr-pi_1397.jpg'\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n",
      "\t [[IteratorGetNext]]\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nFileNotFoundError: [Errno 2] No such file or directory: 'artifacts/data_ingestion/brain-mri/Training/pituitary/Tr-pi_1397.jpg'\nTraceback (most recent call last):\n\n  File \"/Users/lakshitjain/Developer/ML Projects/Brain-Tumor-MRI-Classification/venv/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n    ret = func(*args)\n\n  File \"/Users/lakshitjain/Developer/ML Projects/Brain-Tumor-MRI-Classification/venv/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/Users/lakshitjain/Developer/ML Projects/Brain-Tumor-MRI-Classification/venv/lib/python3.10/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/Users/lakshitjain/Developer/ML Projects/Brain-Tumor-MRI-Classification/venv/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py\", line 264, in _finite_generator\n    yield self._standardize_batch(self.py_dataset[i])\n\n  File \"/Users/lakshitjain/Developer/ML Projects/Brain-Tumor-MRI-Classification/venv/lib/python3.10/site-packages/keras/src/legacy/preprocessing/image.py\", line 68, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"/Users/lakshitjain/Developer/ML Projects/Brain-Tumor-MRI-Classification/venv/lib/python3.10/site-packages/keras/src/legacy/preprocessing/image.py\", line 313, in _get_batches_of_transformed_samples\n    img = image_utils.load_img(\n\n  File \"/Users/lakshitjain/Developer/ML Projects/Brain-Tumor-MRI-Classification/venv/lib/python3.10/site-packages/keras/src/utils/image_utils.py\", line 235, in load_img\n    with open(path, \"rb\") as f:\n\nFileNotFoundError: [Errno 2] No such file or directory: 'artifacts/data_ingestion/brain-mri/Training/pituitary/Tr-pi_1397.jpg'\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_multi_step_on_iterator_3610]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m     training\u001b[38;5;241m.\u001b[39mtrain(callback_list\u001b[38;5;241m=\u001b[39mcallbacks_list)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[0;32mIn[41], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m     training\u001b[38;5;241m.\u001b[39mget_base_model()\n\u001b[1;32m     10\u001b[0m     training\u001b[38;5;241m.\u001b[39mtrain_valid_generator()\n\u001b[0;32m---> 11\u001b[0m     \u001b[43mtraining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallback_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[0;32mIn[40], line 64\u001b[0m, in \u001b[0;36mTraining.train\u001b[0;34m(self, callback_list)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_generator\u001b[38;5;241m.\u001b[39msamples \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_generator\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[1;32m     63\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Started\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 64\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalid_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_list\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_model(\n\u001b[1;32m     74\u001b[0m     path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtrained_model_path,\n\u001b[1;32m     75\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m     76\u001b[0m )\n",
      "File \u001b[0;32m~/Developer/ML Projects/Brain-Tumor-MRI-Classification/venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Developer/ML Projects/Brain-Tumor-MRI-Classification/venv/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nFileNotFoundError: [Errno 2] No such file or directory: 'artifacts/data_ingestion/brain-mri/Training/pituitary/Tr-pi_1397.jpg'\nTraceback (most recent call last):\n\n  File \"/Users/lakshitjain/Developer/ML Projects/Brain-Tumor-MRI-Classification/venv/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n    ret = func(*args)\n\n  File \"/Users/lakshitjain/Developer/ML Projects/Brain-Tumor-MRI-Classification/venv/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/Users/lakshitjain/Developer/ML Projects/Brain-Tumor-MRI-Classification/venv/lib/python3.10/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/Users/lakshitjain/Developer/ML Projects/Brain-Tumor-MRI-Classification/venv/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py\", line 264, in _finite_generator\n    yield self._standardize_batch(self.py_dataset[i])\n\n  File \"/Users/lakshitjain/Developer/ML Projects/Brain-Tumor-MRI-Classification/venv/lib/python3.10/site-packages/keras/src/legacy/preprocessing/image.py\", line 68, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"/Users/lakshitjain/Developer/ML Projects/Brain-Tumor-MRI-Classification/venv/lib/python3.10/site-packages/keras/src/legacy/preprocessing/image.py\", line 313, in _get_batches_of_transformed_samples\n    img = image_utils.load_img(\n\n  File \"/Users/lakshitjain/Developer/ML Projects/Brain-Tumor-MRI-Classification/venv/lib/python3.10/site-packages/keras/src/utils/image_utils.py\", line 235, in load_img\n    with open(path, \"rb\") as f:\n\nFileNotFoundError: [Errno 2] No such file or directory: 'artifacts/data_ingestion/brain-mri/Training/pituitary/Tr-pi_1397.jpg'\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_multi_step_on_iterator_3610]"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config=ConfigurationManager()\n",
    "    prepare_callbacks_config=config.get_prepare_callbacks_config()\n",
    "    prepare_callbacks = PrepareCallback(config=prepare_callbacks_config)\n",
    "    callbacks_list = prepare_callbacks.get_all_callbacks()\n",
    "\n",
    "    training_config=config.get_training_config()\n",
    "    training=Training(config=training_config)\n",
    "    training.get_base_model()\n",
    "    training.train_valid_generator()\n",
    "    training.train(callback_list=callbacks_list)\n",
    "except Exception as e:\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94ddbc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
